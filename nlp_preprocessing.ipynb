{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 4,
>>>>>>> ddfd47b (Similarity comparison)
=======
   "execution_count": 13,
>>>>>>> 05dd611 (WIP)
=======
   "execution_count": 4,
>>>>>>> 0cc7eb6 (Similarity comparison)
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps for preprocessing:\n",
    "    # Tokenization?\n",
    "    # Lower casing\n",
    "    # Stop words removal\n",
    "    # Stemming?\n",
    "    # Lemmatization?\n",
    "    # Bag of words/W2V, CBOW, Skip-gram\n",
    "    # TF-IDF\n",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> 05dd611 (WIP)
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
<<<<<<< HEAD
=======
    "from rapidfuzz import process, fuzz"
>>>>>>> ddfd47b (Similarity comparison)
=======
>>>>>>> 05dd611 (WIP)
=======
    "from rapidfuzz import process, fuzz"
>>>>>>> 0cc7eb6 (Similarity comparison)
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> 05dd611 (WIP)
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_id</th>\n",
       "      <th>abstract_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W2089745446</td>\n",
       "      <td>The research access/impact problem arises beca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W2100379340</td>\n",
       "      <td>Information technology (IT) acceptance researc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W1981083189</td>\n",
       "      <td>Edward O. Wilson, in his famous work, Sociobio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W1560783210</td>\n",
       "      <td>Open access (OA) is free, unrestricted access ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>W2463568293</td>\n",
       "      <td>Open access, open data, open source and other ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       work_id                                   abstract_content\n",
       "0  W2089745446  The research access/impact problem arises beca...\n",
       "1  W2100379340  Information technology (IT) acceptance researc...\n",
       "2  W1981083189  Edward O. Wilson, in his famous work, Sociobio...\n",
       "3  W1560783210  Open access (OA) is free, unrestricted access ...\n",
       "4  W2463568293  Open access, open data, open source and other ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts = pd.read_csv('./abstracts.csv')\n",
    "abstracts.columns = ['work_id', 'abstract_content']\n",
    "abstracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
<<<<<<< HEAD
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_similarity(text1, text2):\n",
    "    # Tokenize and lemmatize the texts\n",
    "    tokens1 = word_tokenize(text1)\n",
    "    tokens2 = word_tokenize(text2)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens1 = [lemmatizer.lemmatize(token) for token in tokens1]\n",
    "    tokens2 = [lemmatizer.lemmatize(token) for token in tokens2]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    tokens1 = [token for token in tokens1 if token not in stop_words]\n",
    "    tokens2 = [token for token in tokens2 if token not in stop_words]\n",
    "\n",
    "    # Create the TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vector1 = vectorizer.fit_transform(tokens1)\n",
    "    vector2 = vectorizer.transform(tokens2)\n",
    "\n",
    "    # Calculate the cosine similarity\n",
    "    similarity = cosine_similarity(vector1, vector2, dense_output=True)\n",
    "    print(similarity)\n",
    "    return similarity\n",
    "\n",
    "def cosine_match(target_ref:str, open_alex_works: list):\n",
    "    similarities = []\n",
    "    for work in open_alex_works:\n",
    "        similarity = text_similarity(target_ref, work)\n",
    "        work_similarity = {\n",
    "                work[0:10]: similarity\n",
    "            }\n",
    "        similarities.append(work_similarity)\n",
    "        # print(work_similarity)\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    Information technology (IT) acceptance researc...\n",
       "2    Edward O. Wilson, in his famous work, Sociobio...\n",
       "Name: abstract_content, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ref = abstracts[0:1]['abstract_content'][0]\n",
    "target_ref\n",
    "open_alex_works = abstracts[1:3]['abstract_content']\n",
    "open_alex_works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "sims = cosine_match(target_ref, open_alex_works)\n",
    "# sims_df = pd.DataFrame(sims)\n",
    "# sims_df\n",
    "# sims"
=======
=======
>>>>>>> 0cc7eb6 (Similarity comparison)
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rapidfuzz_match(extracted_references: list, openalex_works: list, scorer = fuzz.WRatio):\n",
    "    top_match = []\n",
    "    second_match = []\n",
    "    third_match = []\n",
    "    top_names = []\n",
    "    top_scores = []\n",
    "    top_indexes = []\n",
    "    second_scores = []\n",
    "    second_indexes = []\n",
    "    second_names = []\n",
    "    choices = openalex_works\n",
    "    for reference in extracted_references:\n",
    "        # possible scorers are fuzz.WRatio , fuzz.partial_ratio , fuzz.token_set_ratio , fuzz.partial_token_set_ratio , fuzz.token_sort_ratio\n",
    "        top, second, third = process.extract(reference, choices, scorer=scorer, limit=3)\n",
    "        top_score = top[1]\n",
    "        top_index = top[2]\n",
    "        top_name = top[0]\n",
    "        top_names.append(top_name)\n",
    "        top_scores.append(top_score)\n",
    "        top_indexes.append(top_index)\n",
    "        top_match.append(top)\n",
    "        second_match.append(second)\n",
    "        second_score = second[1]\n",
    "        second_index = second[2]\n",
    "        second_name = second[0]\n",
    "        second_names.append(second_name)\n",
    "        second_scores.append(second_score)\n",
    "        second_indexes.append(second_index)\n",
    "        third_match.append(third)\n",
    "    matched_df = pd.DataFrame(list(zip(extracted_references, top_match,top_names, top_scores, top_indexes, second_match, second_names, second_scores, second_indexes, third_match)), columns=['extracted_reference', 'top_match', 'top_names', 'top_scores', 'top_indexes', 'second_match', 'second_names', 'second_scores', 'second_indexes', 'third_match'])\n",
    "    return matched_df"
<<<<<<< HEAD
>>>>>>> ddfd47b (Similarity comparison)
=======
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_similarity(text1, text2):\n",
    "    # Tokenize and lemmatize the texts\n",
    "    tokens1 = word_tokenize(text1)\n",
    "    tokens2 = word_tokenize(text2)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens1 = [lemmatizer.lemmatize(token) for token in tokens1]\n",
    "    tokens2 = [lemmatizer.lemmatize(token) for token in tokens2]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    tokens1 = [token for token in tokens1 if token not in stop_words]\n",
    "    tokens2 = [token for token in tokens2 if token not in stop_words]\n",
    "\n",
    "    # Create the TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vector1 = vectorizer.fit_transform(tokens1)\n",
    "    vector2 = vectorizer.transform(tokens2)\n",
    "\n",
    "    # Calculate the cosine similarity\n",
    "    similarity = cosine_similarity(vector1, vector2, dense_output=True)\n",
    "    print(similarity)\n",
    "    return similarity\n",
    "\n",
    "def cosine_match(target_ref:str, open_alex_works: list):\n",
    "    similarities = []\n",
    "    for work in open_alex_works:\n",
    "        similarity = text_similarity(target_ref, work)\n",
    "        work_similarity = {\n",
    "                work[0:10]: similarity\n",
    "            }\n",
    "        similarities.append(work_similarity)\n",
    "        # print(work_similarity)\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    Information technology (IT) acceptance researc...\n",
       "2    Edward O. Wilson, in his famous work, Sociobio...\n",
       "Name: abstract_content, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ref = abstracts[0:1]['abstract_content'][0]\n",
    "target_ref\n",
    "open_alex_works = abstracts[1:3]['abstract_content']\n",
    "open_alex_works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "sims = cosine_match(target_ref, open_alex_works)\n",
    "# sims_df = pd.DataFrame(sims)\n",
    "# sims_df\n",
    "# sims"
>>>>>>> 05dd611 (WIP)
=======
>>>>>>> 0cc7eb6 (Similarity comparison)
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reviewerSelection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
